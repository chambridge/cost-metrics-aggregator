package processor

import (
	"context"
	"encoding/csv"
	"fmt"
	"log"
	"strconv"
	"time"

	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgxpool"
)

// NodeData represents distinct node information for an hourly interval
type NodeData struct {
	NodeName         string
	ResourceID       string
	NodeRole         string
	CapacityCPUCores float64
	IntervalStart    time.Time
	ClusterID        string
}

// DailySummaryData represents aggregated data for daily_summary
type DailySummaryData struct {
	NodeID     uuid.UUID
	Date       time.Time
	CoreCount  int
	TotalHours int
}

// ProcessCSV processes a CSV reader, extracting distinct node data and inserting into nodes, metrics, and daily_summary
func ProcessCSV(ctx context.Context, db *pgxpool.Pool, reader *csv.Reader, clusterID string) error {
	// Read all records
	records, err := reader.ReadAll()
	if err != nil {
		return fmt.Errorf("failed to read CSV records: %w", err)
	}

	// Map header indices
	if len(records) < 1 {
		return fmt.Errorf("empty CSV file")
	}
	headers := records[0]
	headerIndices := make(map[string]int)
	for i, h := range headers {
		headerIndices[h] = i
	}

	// Collect distinct node data by node and interval_start
	nodeDataMap := make(map[string]NodeData)
	// Aggregate daily summary data by node_id and date
	dailySummaryMap := make(map[string]*DailySummaryData)

	for _, record := range records[1:] {
		if len(record) < len(headers) {
			log.Printf("Skipping invalid record: %v", record)
			continue
		}

		intervalStartStr := record[headerIndices["interval_start"]]
		nodeName := record[headerIndices["node"]]
		resourceID := record[headerIndices["resource_id"]]
		nodeRole := record[headerIndices["node_role"]]
		capacityCPUStr := record[headerIndices["node_capacity_cpu_cores"]]

		intervalStart, err := time.Parse(time.RFC3339, intervalStartStr)
		if err != nil {
			log.Printf("Skipping record: invalid interval_start %s: %v", intervalStartStr, err)
			continue
		}

		capacityCPU, err := strconv.ParseFloat(capacityCPUStr, 64)
		if err != nil {
			log.Printf("Skipping record: invalid node_capacity_cpu_cores %s: %v", capacityCPUStr, err)
			continue
		}

		// Use nodeName and interval_start as the key for distinct node data
		key := fmt.Sprintf("%s|%s", nodeName, intervalStartStr)
		nodeDataMap[key] = NodeData{
			NodeName:         nodeName,
			ResourceID:       resourceID,
			NodeRole:         nodeRole,
			CapacityCPUCores: capacityCPU,
			IntervalStart:    intervalStart,
			ClusterID:        clusterID,
		}
	}

	// Insert node data into nodes, metrics, and daily_summary tables
	for _, data := range nodeDataMap {
		clusterUUID, err := uuid.Parse(data.ClusterID)
		if err != nil {
			log.Printf("Skipping node %s: invalid cluster_id %s: %v", data.NodeName, data.ClusterID, err)
			continue
		}

		// Prepare identifier (NULL if resource_id is empty)
		var identifier *string
		if data.ResourceID != "" {
			identifier = &data.ResourceID
		}

		// Insert into nodes table (id is generated by DEFAULT gen_random_uuid())
		nodeType := data.NodeRole
		if nodeType == "" {
			nodeType = "worker" // Default
		}
		var nodeID uuid.UUID
		err = db.QueryRow(ctx, `
			INSERT INTO nodes (cluster_id, name, identifier, type)
			VALUES ($1, $2, $3, $4)
			ON CONFLICT (identifier) DO UPDATE
			SET name = EXCLUDED.name,
				cluster_id = EXCLUDED.cluster_id,
				type = EXCLUDED.type
			RETURNING id
		`, clusterUUID, data.NodeName, identifier, nodeType).Scan(&nodeID)
		if err != nil {
			log.Printf("Failed to insert/update node %s: %v", data.NodeName, err)
			continue
		}

		// Insert into metrics table
		_, err = db.Exec(ctx, `
			INSERT INTO metrics (node_id, timestamp, core_count, cluster_id)
			VALUES ($1, $2, $3, $4)
			ON CONFLICT DO NOTHING
		`, nodeID, data.IntervalStart, int(data.CapacityCPUCores), clusterUUID)
		if err != nil {
			log.Printf("Failed to insert metrics for node %s at %s: %v", data.NodeName, data.IntervalStart, err)
			continue
		}

		// Aggregate daily summary data
		date := data.IntervalStart.Truncate(24 * time.Hour)
		summaryKey := fmt.Sprintf("%s|%s|%d", nodeID.String(), date.Format("2006-01-02"), int(data.CapacityCPUCores))
		if summary, exists := dailySummaryMap[summaryKey]; exists {
			summary.TotalHours++
		} else {
			dailySummaryMap[summaryKey] = &DailySummaryData{
				NodeID:     nodeID,
				Date:       date,
				CoreCount:  int(data.CapacityCPUCores),
				TotalHours: 1,
			}
		}
	}

	// Insert into daily_summary table
	for _, summary := range dailySummaryMap {
		_, err := db.Exec(ctx, `
			INSERT INTO daily_summary (node_id, date, core_count, total_hours)
			VALUES ($1, $2, $3, $4)
			ON CONFLICT (node_id, date, core_count) DO UPDATE
			SET total_hours = GREATEST(daily_summary.total_hours, EXCLUDED.total_hours)
		`, summary.NodeID, summary.Date, summary.CoreCount, summary.TotalHours)
		if err != nil {
			log.Printf("Failed to insert daily_summary for node %s on %s: %v", summary.NodeID, summary.Date, err)
			continue
		}
	}

	return nil
}
